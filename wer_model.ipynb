{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1HTr1yjWvaLpoD5lD0oooWoZMSF5v0i6C",
      "authorship_tag": "ABX9TyNiETLKeCHPzPZPzLtWf+wp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaditiash/WERmodel_MLproject/blob/main/wer_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twMwhVOhPmiM",
        "outputId": "c55551bd-9e64-4163-85ec-e29fb365e12f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "_BTXtQ-aC95T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sliyWS0posTi",
        "outputId": "266d7df3-8226-49d4-adf5-1ba4e5c6d642"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7GVSvzmrMyR",
        "outputId": "04bfd492-42cf-4afa-8513-7f000cd47cb5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk67awI-sS1M",
        "outputId": "c3606230-f739-4fc6-a801-e75dc90bd55e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def calculate_wer(ref_file, trans_file):\n",
        "    with open(ref_file, 'r') as ref, open(trans_file, 'r') as trans:\n",
        "        ref_text = ref.read().strip().lower()\n",
        "        trans_text = trans.read().strip().lower()\n",
        "\n",
        "    ref_words = nltk.tokenize.word_tokenize(ref_text)\n",
        "    trans_words = nltk.tokenize.word_tokenize(trans_text)\n",
        "\n",
        "    # Calculate WER\n",
        "    ref_set = set(ref_words)\n",
        "    trans_set = set(trans_words)\n",
        "    total_words = len(ref_words)\n",
        "\n",
        "    substitutions = nltk.edit_distance(ref_words, trans_words)\n",
        "\n",
        "    deletions = len(ref_set - trans_set)\n",
        "\n",
        "    insertions = len(trans_set - ref_set)\n",
        "\n",
        "    wer = (substitutions + deletions + insertions) / total_words\n",
        "\n",
        "    return wer\n",
        "\n",
        "\n",
        "reference_file = '/content/drive/MyDrive/ML project/txt_files/eng3.txt'\n",
        "translated_file = '/content/drive/MyDrive/ML project/txt_files/rec3.txt'\n",
        "wer_score = calculate_wer(reference_file, translated_file)\n",
        "print(f\"Word Error Rate: {wer_score:.2f}%\")\n"
      ],
      "metadata": {
        "id": "vanTfCNLR-iz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f220af2-3d47-4eff-df23-a2698d220c07"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Error Rate: 0.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def calculate_wer(ref_file, trans_file):\n",
        "    with open(ref_file, 'r') as ref, open(trans_file, 'r') as trans:\n",
        "        ref_text = ref.read().strip().lower()\n",
        "        trans_text = trans.read().strip().lower()\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    ref_words = nltk.word_tokenize(ref_text)\n",
        "    trans_words = nltk.word_tokenize(trans_text)\n",
        "\n",
        "    ref_lemmas = [lemmatizer.lemmatize(word) for word in ref_words]\n",
        "    trans_lemmas = [lemmatizer.lemmatize(word) for word in trans_words]\n",
        "\n",
        "    ref_set = set(ref_lemmas)\n",
        "    trans_set = set(trans_lemmas)\n",
        "\n",
        "    deletions = len(ref_set - trans_set)\n",
        "    insertions = len(trans_set - ref_set)\n",
        "\n",
        "    substitutions = nltk.edit_distance(ref_lemmas, trans_lemmas) - deletions\n",
        "\n",
        "    total_words = len(ref_words)\n",
        "    wer = (deletions + insertions + substitutions) / total_words\n",
        "\n",
        "    return wer\n",
        "\n",
        "# Example usage\n",
        "reference_file = '/content/drive/MyDrive/ML project/txt_files/eng3.txt'\n",
        "translated_file = '/content/drive/MyDrive/ML project/txt_files/rec3.txt'\n",
        "wer_score = calculate_wer(reference_file, translated_file)\n",
        "print(f\"Word Error Rate: {wer_score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPKLPjYZq9jJ",
        "outputId": "26dc9a6e-6d21-46fe-bce5-ccea991973f7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Error Rate: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "def calculate_wer(ref_file, trans_file):\n",
        "    with open(ref_file, 'r') as ref, open(trans_file, 'r') as trans:\n",
        "        ref_text = ref.read().lower()\n",
        "        trans_text = trans.read().lower()\n",
        "\n",
        "    # Tokenization\n",
        "    ref_tokens = word_tokenize(ref_text)\n",
        "    trans_tokens = word_tokenize(trans_text)\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    ref_lemmas = [lemmatizer.lemmatize(token) for token in ref_tokens]\n",
        "    trans_lemmas = [lemmatizer.lemmatize(token) for token in trans_tokens]\n",
        "\n",
        "    # Remove punctuation\n",
        "    punctuations = string.punctuation\n",
        "    ref_no_punct = [token for token in ref_lemmas if token not in punctuations]\n",
        "    trans_no_punct = [token for token in trans_lemmas if token not in punctuations]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    ref_no_stop = [token for token in ref_no_punct if token not in stop_words]\n",
        "    trans_no_stop = [token for token in trans_no_punct if token not in stop_words]\n",
        "\n",
        "    # Calculate WER\n",
        "    ref_set = set(ref_no_stop)\n",
        "    trans_set = set(trans_no_stop)\n",
        "\n",
        "    deletions = len(ref_set - trans_set)\n",
        "\n",
        "    insertions = len(trans_set - ref_set)\n",
        "\n",
        "    substitutions = nltk.edit_distance(ref_no_stop, trans_no_stop) - deletions\n",
        "\n",
        "    total_words = len(ref_no_stop)\n",
        "    wer = (deletions + insertions + substitutions) / total_words\n",
        "\n",
        "    return wer\n",
        "\n",
        "# Example usage\n",
        "reference_file = '/content/drive/MyDrive/ML project/txt_files/eng3.txt'\n",
        "translated_file = '/content/drive/MyDrive/ML project/txt_files/rec3.txt'\n",
        "wer_score = calculate_wer(reference_file, translated_file)\n",
        "print(f\"Word Error Rate: {wer_score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmmlsLznsGKY",
        "outputId": "9b29953d-dc10-45ee-ebce-b199f6f488d0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Error Rate: 0.85\n"
          ]
        }
      ]
    }
  ]
}